# -----------------------------------------------------------------------------
# DEFAULT CONFIGURATION TEMPLATE FOR THE TRAINING PIPELINE (COMPLETE)
# -----------------------------------------------------------------------------
# This file defines all default parameters. A user can override any of these
# settings in their own `config.yaml` file.

# --- Pipeline & Data Settings (User MUST provide these) ---
# The user's config.yaml will provide these values.
# pipeline:
#   output_dir: "/path/to/training_output"
# ... etc ...

# --- Optional Data & Pipeline Settings ---
data:
  shuffle: true
  num_workers: 4
  val_ratio: 0.1
  test_ratio: 0.1
  metrics_mode: "1" # "1" for per-timestep scores
  metrics_list: ['csi', 'pod', 'sucr', 'bias', 'fss']
  threshold_list: [0.1, 2.5, 7.6, 16.0]

layout:
  layout: "NTHWC"

# --- Training & Optimization Settings ---
optim:
  total_batch_size: 16
  micro_batch_size: 2
  max_epochs: 50
  method: "adamw"
  lr: 0.0001
  wd: 0.01
  betas: [0.9, 0.999]
  monitor: "val_loss"
  warmup_percentage: 0.1
  gradient_clip_val: 1.0

trainer:
  precision: 32
  log_every_n_steps: 50

# --- Visualization Settings ---
visualization:
  denorm_clip_value: 100.0
  colorbar_label: "Precipitation (mm/hr)"
  boundaries: [0.0, 0.1, 2.5, 7.6, 16.0, 50.0, 100.0]
  cmap_data:
    - [1.0, 1.0, 1.0]
    - [0.7, 0.85, 0.95]
    - [0.2, 0.6, 0.85]
    - [0.5, 0.85, 0.5]
    - [1.0, 0.8, 0.2]
    - [0.9, 0.0, 0.0]

# -----------------------------------------------------------------------------
# MODEL ARCHITECTURE DEFAULTS
# -----------------------------------------------------------------------------
model:
  # --- VAE Model ---
  vae:
    pretrained_ckpt_path: null
    in_channels: 1
    out_channels: 1
    latent_channels: 64
    down_block_types: ['DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D']
    up_block_types: ['UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D']
    block_out_channels: [128, 256, 512, 512]
    layers_per_block: 2
    act_fn: 'silu'
    norm_num_groups: 32
    loss:
      disc_start: 50001
      kl_weight: 1.0e-6
      disc_weight: 0.5
      perceptual_weight: 0.0
      disc_in_channels: 1

  # --- Alignment Model (NoisyCuboidTransformerEncoder) ---
  align:
    model_ckpt_path: null
    alignment_type: "avg_x"
    guide_scale: 50.0
    model_type: "cuboid"
    model_args:
      input_shape: null # Auto-filled
      out_channels: 1
      base_units: 128
      scale_alpha: 1.0
      depth: [1, 1]
      downsample: 2
      downsample_type: "patch_merge"
      block_attn_patterns: "axial"
      num_heads: 4
      attn_drop: 0.1
      proj_drop: 0.1
      ffn_drop: 0.1
      ffn_activation: "gelu"
      gated_ffn: false
      norm_layer: "layer_norm"
      use_inter_ffn: true
      pos_embed_type: "t+h+w"
      padding_type: "zeros"
      checkpoint_level: 0
      use_relative_pos: true
      self_attn_use_final_proj: true
      num_global_vectors: 0
      use_global_vector_ffn: true
      use_global_self_attn: false
      separate_global_qkv: false
      global_dim_ratio: 1
      time_embed_channels_mult: 4
      time_embed_use_scale_shift_norm: false
      time_embed_dropout: 0.0
      pool: "attention"
      readout_seq: true
      out_len: null # Auto-filled from layout.out_len

  # --- PreDiff (CuboidTransformerUNet) ---
  latent_model:
    input_shape: null # Auto-filled
    target_shape: null # Auto-filled
    base_units: 256
    scale_alpha: 1.0
    num_heads: 4
    attn_drop: 0.1
    proj_drop: 0.1
    ffn_drop: 0.1
    downsample: 2
    downsample_type: "patch_merge"
    upsample_type: "upsample"
    upsample_kernel_size: 3
    depth: [4, 4]
    block_attn_patterns: "axial"
    num_global_vectors: 0
    use_global_vector_ffn: false
    use_global_self_attn: true
    separate_global_qkv: true
    global_dim_ratio: 1
    ffn_activation: "gelu"
    gated_ffn: false
    norm_layer: "layer_norm"
    padding_type: "zeros"
    pos_embed_type: "t+h+w"
    checkpoint_level: 0
    use_relative_pos: true
    self_attn_use_final_proj: true
    time_embed_channels_mult: 4
    time_embed_use_scale_shift_norm: false
    time_embed_dropout: 0.0
    unet_res_connect: true

  # --- Diffusion Process ---
  diffusion:
    layout: "NTHWC" # Must match global layout
    data_shape: null # Auto-filled
    timesteps: 1000
    beta_schedule: "linear"
    use_ema: true
    loss_type: "l2"
    linear_start: 1.0e-4
    linear_end: 2.0e-2
    cosine_s: 8.0e-3
    given_betas: null
    original_elbo_weight: 0.0
    v_posterior: 0.0
    l_simple_weight: 1.0
    parameterization: "eps"
    learn_logvar: false
    logvar_init: 0.0
    latent_shape: null # Auto-filled
    cond_stage_model: "__is_first_stage__" # Internal flag
    num_timesteps_cond: null
    cond_stage_trainable: false
    cond_stage_forward: null
    scale_by_std: false
    scale_factor: 1.0